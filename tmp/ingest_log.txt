============================================================
Parallel Chunked Ingestion Pipeline
============================================================
Cleared database
Processing 1 page(s) with 5 chunks each
Parallel extraction: 5 API calls per page

  Processing 32585599286500cb_page_1.png...
    [basic] ✓ 17.8s
    [FLUSHED] basic → company_id=1, doc_id=1
    [market] ✓ 27.1s
    [shareholding] ✓ 44.5s
    [forecasts] ✗ Request timed out.
    [qualitative] ✗ Request timed out.
Traceback (most recent call last):
  File "/Users/tomkremer/Documents/Jobs/GigaML/gigaml/ingest.py", line 310, in <module>
    
  File "/Users/tomkremer/Documents/Jobs/GigaML/gigaml/ingest.py", line 296, in main
    
  File "/Users/tomkremer/Documents/Jobs/GigaML/gigaml/ingest.py", line 252, in process_page
    company_id, doc_id = db.flush_basic(filename, result["data"])
  File "/Users/tomkremer/Documents/Jobs/GigaML/gigaml/ingest.py", line 159, in flush_market
    cur.execute("INSERT INTO metrics (company_id, document_id, field_name, value, unit) VALUES (?,?,?,?,?)",
sqlite3.OperationalError: attempt to write a readonly database
